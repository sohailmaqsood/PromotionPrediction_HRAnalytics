# -*- coding: utf-8 -*-
"""HR_Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y2FWyW-cEjCOpqg8tf9iRvjekU9EtR-4

**Problem Statement can be found on the given link**
https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/#MySubmissions
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

#keeping the original copies of data sets for future reference
train_orig = train.copy()
test_orig  = test.copy()

#let explore the data
#check for null values in train 
train.isnull().sum()

test.isnull().sum()

#checking the information about the train file
train.info()

#Lets do some ETA
plt.figure(1)
plt.subplots_adjust(hspace=0.5)
plt.subplot(231)
train['department'].value_counts(normalize=True).plot.bar(figsize=(20,10),title='Departments')
plt.subplot(232)
train['region'].value_counts(normalize=True).plot.bar(figsize=(20,10),title='Region')
plt.subplot(233)
train['education'].value_counts(normalize=True).plot.bar(figsize=(20,10),title='Education')
plt.subplot(234)
train['recruitment_channel'].value_counts(normalize=True).plot.bar(figsize=(20,10),title='Recruitment Channel')
plt.subplot(235)
train['gender'].value_counts(normalize=True).plot.bar(figsize=(20,10),title='Gender')

train['no_of_trainings'].value_counts(normalize=True).plot.bar()

sns.distplot(train['age'],hist=True,bins=10)

"""Plot looks normally distributed"""

bins = [i for i in range(0,65,5)]
train['age_bin'] = pd.cut(train['age'],bins)

bins = [i for i in range(0,65,5)]
test['age_bin'] = pd.cut(test['age'],bins)

train['age_bin'].value_counts()

age = pd.crosstab(train['age_bin'],train['is_promoted'])
age.div(age.sum(1).astype(float),axis=0).plot.bar(stacked=True)

"""age group of 25,30,35 seems to be getting promoted more"""

train.drop(['age'],axis=1,inplace=True)

test.drop(['age'],axis=1,inplace=True)

train.education.fillna(train['education'].mode()[0],inplace=True) #filling the nulls with Mode of data
test.education.fillna(test['education'].mode()[0],inplace=True)

train.info()

train.previous_year_rating.value_counts()

"""there are two options to fill the missing ratings. **Either fill it with the mode i.e. 3 ** or ** fill it with the rating based on other parameters in the data**"""

#lets first check the effect of previous rating on the outcome

prvrat = pd.crosstab(train['previous_year_rating'],train['is_promoted'])
prvrat.div(prvrat.sum(1).astype(float),axis=0).plot.bar(stacked=True)

"""Previous year rating seems to significant impact on promotion for this year"""

#lets try replacing this with the mode
train.previous_year_rating.fillna(train.previous_year_rating.mode()[0],inplace=True)
test.previous_year_rating.fillna(test.previous_year_rating.mode()[0],inplace=True)

train.isnull().sum()

train.info()

train = train.drop('employee_id',1)
test = test.drop('employee_id',1)

train = pd.get_dummies(train)

test = pd.get_dummies(test)

train.info()

y = train.is_promoted
x = train.drop('is_promoted',1)

x.columns

"""*Splitting the dataset*"""

from sklearn.model_selection import train_test_split
x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.30)

#check for class balance
y_train.value_counts()

y_val.value_counts()

"""***Modelling Starts from here***

**Logistic Model**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model = LogisticRegression()
model.fit(x_train,y_train)

pred_y = model.predict(x_val)

accuracy_score(y_val,pred_y)

from sklearn.metrics import f1_score
f1_score(y_val,pred_y)

"""**Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(class_weight='balanced')
classifier.fit(x_train,y_train)

tree_pred = classifier.predict(x_val)

f1_score(y_val,tree_pred)

"""**testing both models using Stratified K fold validations**"""

import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)
deciarr = 0
i = 1
for train_index, test_index in skf.split(x,y):
  #print("Train:",train_index,"Validation:",test_index)
  x_train,x_test = x.iloc[train_index],x.iloc[test_index]
  y_train,y_test = y.iloc[train_index],y.iloc[test_index]
  model = LogisticRegression(random_state=1,class_weight='balanced')
  model.fit(x_train,y_train)
  logipred = model.predict(x_test)
  print('{} iteration for Logistic Regression has f1 score of {}'.format(i,f1_score(y_test,logipred)))
  classifer = DecisionTreeClassifier(class_weight='balanced')
  classifier.fit(x_train,y_train)
  decipred = classifier.predict(x_test)
  print('{} iteration for Decision Tree has f1 score of {}'.format(i,f1_score(y_test,decipred)))
  i += 1
  deciarr += f1_score(y_test,decipred)

deciarr/10

#this cell is for getting the confusion matrix of the model
from sklearn.metrics import confusion_matrix

conf_mat = confusion_matrix(y_true=y_val,y_pred=pred_y)
print('Confusion Matrix for Logistic Model is \n',conf_mat)
plt.matshow(conf_mat)
plt.colorbar()

conf_mat = confusion_matrix(y_true=y_val,y_pred=tree_pred)
print('Confusion matrix for Decision Tree is \n',conf_mat)

"""**trying out balancing techniques**"""

#since the data is higly imbalance, lets try some balancing techniques
#UnderSampling
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=0)
x_res,y_res = rus.fit_resample(x,y)
x_res = pd.DataFrame(x_res)
x_res.columns = x.columns

y_res = pd.Series(y_res)

import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)
i = 1
deciscr = 0
for train_index, test_index in skf.split(x_res,y_res):
  #print("Train:",train_index,"Validation:",test_index)
  x_train,x_test = x_res.iloc[train_index],x_res.iloc[test_index]
  y_train,y_test = y_res.iloc[train_index],y_res.iloc[test_index]
  classifer = DecisionTreeClassifier()
  classifier.fit(x_train,y_train)
  decipred = classifier.predict(x_test)
  print('{} iteration for Decision Tree has f1 score of {}'.format(i,f1_score(y_test,decipred)))
  i += 1
  deciscr += f1_score(y_test,decipred)
deciscr/10

#lets train the model on undersampled dataset and see the results
classifier = DecisionTreeClassifier(class_weight='balanced')
classifier.fit(x_res,y_res)

#SMOTEOverSampling
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek
smote = SMOTE(random_state=42)
smto = SMOTETomek(ratio='auto',smote=smote)
x_res,y_res = smto.fit_resample(x,y)


x_res = pd.DataFrame(x_res)
x_res.columns = x.columns

y_res = pd.Series(y_res)

x_res.shape

import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)
i = 1
deciscr = 0
for train_index, test_index in skf.split(x_res,y_res):
  #print("Train:",train_index,"Validation:",test_index)
  x_train,x_test = x_res.iloc[train_index],x_res.iloc[test_index]
  y_train,y_test = y_res.iloc[train_index],y_res.iloc[test_index]
  classifer = DecisionTreeClassifier(class_weight='balanced')
  classifier.fit(x_train,y_train)
  decipred = classifier.predict(x_test)
  print('{} iteration for Decision Tree has f1 score of {}'.format(i,f1_score(y_test,decipred)))
  print('{} iteration for Decision Tree has accuracy score of {}'.format(i,accuracy_score(y_test,decipred)))
  i += 1
  deciscr += f1_score(y_test,decipred)
deciscr/10

#this for predicting results in the actual test dataset using Logistic Model
pred_test = model.predict(test)

pred_test_deci = classifier.predict(test)

sub = pd.read_csv('sample_submission_orig.csv')

sub['is_promoted'] = pred_test_deci
sub['employee_id'] = test_orig['employee_id']
sub.columns

sub.to_csv('sample_submission.csv',columns=['employee_id','is_promoted'],index_label=False)

sub.columns

